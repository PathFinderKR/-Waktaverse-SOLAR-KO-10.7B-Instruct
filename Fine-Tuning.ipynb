{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Login to Hugging Face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c2f9104252a4bb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/pathfinder/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=token, # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:35.373483Z",
     "start_time": "2024-04-03T05:12:34.821951Z"
    }
   },
   "id": "4eee58c68eb02d85",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_name = \"waktaverse-gemma-ko-7b-it\" # ADD YOUR MODEL NAME HERE\n",
    "username = \"PathFinderKR\"  # ADD YOUR USERNAME HERE\n",
    "repo_id = f\"{username}/{model_name}\"  # repository id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:35.376991Z",
     "start_time": "2024-04-03T05:12:35.374687Z"
    }
   },
   "id": "cd4c26069d51a597",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloads"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78114e8f78ee19ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install transformers\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft\n",
    "#!pip install trl\n",
    "#!pip install accelerate\n",
    "#!pip install datasets\n",
    "#!pip install scikit-learn\n",
    "#!pip install packaging\n",
    "#!pip install ninja\n",
    "#!pip install flash-attn --no-build-isolation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:35.385105Z",
     "start_time": "2024-04-03T05:12:35.377879Z"
    }
   },
   "id": "ce6454f84604b9e4",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "685ac8c0e05ac872"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# datasets\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:38.679428Z",
     "start_time": "2024-04-03T05:12:35.386649Z"
    }
   },
   "id": "f8c108abcd576306",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e512d30c21d105c2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:38.683428Z",
     "start_time": "2024-04-03T05:12:38.680567Z"
    }
   },
   "id": "fdf3a2a4e0e31554",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ce31a12e172a64"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenizer arguments\n",
    "max_length = 512 # maximum length of the text that can go to the model\n",
    "padding = \"max_length\" # padding strategy: \"longest\", \"max_length\", \"do_not_pad\"\n",
    "truncation = True # truncate the text if it exceeds the maximum length\n",
    "\n",
    "# model arguments\n",
    "max_new_tokens=500 # maximum number of tokens to generate\n",
    "\n",
    "# mixed precision\n",
    "dtype = torch.bfloat16 # data type\n",
    "\n",
    "# quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # load model in 4-bit\n",
    "    bnb_4bit_compute_dtype=dtype, # compute in (data type)\n",
    "    bnb_4bit_quant_type=\"nf4\", # quantize to 4-bit\n",
    "    bnb_4bit_use_doulbe_quant=False # use double quantization\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type = \"CAUSAL_LM\", # task type\n",
    "    r = 8, # rank\n",
    "    target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # target modules\n",
    "    lora_alpha = 16, # alpha\n",
    "    lora_dropout = 0.1, # dropout\n",
    "    bias=\"none\", # bias\n",
    ")\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\", # output directory\n",
    "    logging_dir=\"./logs\", # logging directory\n",
    "    save_strategy=\"epoch\", # save strategy\n",
    "    logging_strategy=\"steps\", # logging strategy\n",
    "    logging_steps=10, # logging steps\n",
    "    save_total_limit=1, # save total limit\n",
    "    \n",
    "    learning_rate=2e-5, # learning rate\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    per_device_train_batch_size=1, # training batch size\n",
    "    per_device_eval_batch_size=1, # evaluation batch size\n",
    "    optim=\"adamw_torch\", # optimizer\n",
    "    weight_decay=0.1, # weight decay\n",
    "    lr_scheduler_type=\"cosine\", # learning rate scheduler\n",
    "    seed=42 # seed\n",
    ")\n",
    "\n",
    "# SFTTrainer arguments\n",
    "max_seq_length = 512 # maximum sequence length"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:38.697234Z",
     "start_time": "2024-04-03T05:12:38.684428Z"
    }
   },
   "id": "dc1a272a1c0b7ce4",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5d3b7c47ea29a3d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Model List\n",
    "\n",
    "# gemma variants\n",
    "# \"google/gemma-7b-it\" // downloaded\n",
    "# \"PathFinderKR/waktaverse-gemma-ko-7b-it\"\n",
    "# \"beomi/gemma-ko-7b\"\n",
    "\n",
    "# llama2 variants\n",
    "# \"meta-llama/Llama-2-7b-chat-hf\" // downloaded\n",
    "# \"PathFinderKR/waktaverse-Llama-2-ko-7b-it\"\n",
    "# \"beomi/KoAlpaca-Polyglot-5.8B\" // downloaded\n",
    "# \"beomi/open-llama-2-ko-7b\"\n",
    "\n",
    "# solar variants\n",
    "# \"chihoonlee10/T3Q-ko-solar-dpo-v4.0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:38.700344Z",
     "start_time": "2024-04-03T05:12:38.698210Z"
    }
   },
   "id": "70d2fbe5c0980a54",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-7b-it\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:38.709455Z",
     "start_time": "2024-04-03T05:12:38.701521Z"
    }
   },
   "id": "eb8160d8919cdaeb",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:12:39.314478Z",
     "start_time": "2024-04-03T05:12:38.710506Z"
    }
   },
   "id": "4ebbf883fce9aa8",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26bc25e4d21246fd8b4f601206138348"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:05.998596Z",
     "start_time": "2024-04-03T05:12:39.316171Z"
    }
   },
   "id": "978055543828a014",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4042d4b7cd2d0fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"MarkrAI/KoCommercial-Dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:11.999066Z",
     "start_time": "2024-04-03T05:13:05.999566Z"
    }
   },
   "id": "4373ef1c6a85e5e2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input', 'instruction', 'output'],\n        num_rows: 175454\n    })\n})"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:12.003604Z",
     "start_time": "2024-04-03T05:13:12.000108Z"
    }
   },
   "id": "aabd9487c235d581",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'input': '',\n 'instruction': '보드 게임 스피너는 $A$, $B$, $C$로 표시된 세 부분으로 나뉩니다. 스피너가 $A$에 떨어질 확률은 $\\\\frac{1}{3}$이고, 스피너가 $B$에 떨어질 확률은 $\\\\frac{5}{12}$입니다.  스피너가 $C$에 착륙할 확률은 얼마입니까? 답을 공통 분수로 표현하세요.',\n 'output': '모든 가능한 결과의 확률의 합이 1$이므로, 스피너가 $C$에 착륙할 확률을 구하려면 스피너가 $A$와 $B$에 착륙할 확률을 1$에서 빼야 합니다. 이를 방정식으로 쓸 수 있습니다: $P(C) = 1 - P(A) - P(B)$. P(A) = \\\\frac{1}{3}$, $P(B) = \\\\frac{5}{12}$라는 것을 알고 있으므로 이 값을 방정식에 대입하여 단순화할 수 있습니다. 결과는 다음과 같습니다: P(C) = 1 - \\\\frac{1}{3} - frac{5}{12} = \\\\frac{12}{12} - frac{4}{12} - frac{5}{12} = \\\\frac{3}{12}$. 분자와 분모를 $3$로 나누면 이 분수를 줄일 수 있습니다: P(C) = \\\\frac{1}{4}$입니다.'}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:12.012383Z",
     "start_time": "2024-04-03T05:13:12.004808Z"
    }
   },
   "id": "760399232efac9ea",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Concatenate the 'instruction' and 'output' fields for each example in the batch\n",
    "    concatenated_texts = [instruction + ' ' + output for instruction, output in zip(examples['instruction'], examples['output'])]\n",
    "    # Tokenize the concatenated texts\n",
    "    return tokenizer(concatenated_texts, padding=padding, truncation=truncation, max_length=max_length)\n",
    "\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:12.350883Z",
     "start_time": "2024-04-03T05:13:12.013338Z"
    }
   },
   "id": "5f8905546b688101",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '', 'instruction': '보드 게임 스피너는 $A$, $B$, $C$로 표시된 세 부분으로 나뉩니다. 스피너가 $A$에 떨어질 확률은 $\\\\frac{1}{3}$이고, 스피너가 $B$에 떨어질 확률은 $\\\\frac{5}{12}$입니다.  스피너가 $C$에 착륙할 확률은 얼마입니까? 답을 공통 분수로 표현하세요.', 'output': '모든 가능한 결과의 확률의 합이 1$이므로, 스피너가 $C$에 착륙할 확률을 구하려면 스피너가 $A$와 $B$에 착륙할 확률을 1$에서 빼야 합니다. 이를 방정식으로 쓸 수 있습니다: $P(C) = 1 - P(A) - P(B)$. P(A) = \\\\frac{1}{3}$, $P(B) = \\\\frac{5}{12}$라는 것을 알고 있으므로 이 값을 방정식에 대입하여 단순화할 수 있습니다. 결과는 다음과 같습니다: P(C) = 1 - \\\\frac{1}{3} - frac{5}{12} = \\\\frac{12}{12} - frac{4}{12} - frac{5}{12} = \\\\frac{3}{12}$. 분자와 분모를 $3$로 나누면 이 분수를 줄일 수 있습니다: P(C) = \\\\frac{1}{4}$입니다.', 'input_ids': [2, 237036, 237135, 144280, 32275, 238810, 239632, 236214, 697, 235280, 5750, 697, 235305, 5750, 697, 235288, 235323, 236375, 100280, 236569, 238602, 48740, 43761, 238304, 26291, 38585, 255221, 12957, 235265, 32275, 238810, 239632, 236361, 697, 235280, 235323, 236179, 235248, 243433, 236770, 239574, 69781, 243156, 236648, 1467, 2552, 235282, 235274, 1214, 235304, 1208, 224907, 235269, 32275, 238810, 239632, 236361, 697, 235305, 235323, 236179, 235248, 243433, 236770, 239574, 69781, 243156, 236648, 1467, 2552, 235282, 235308, 1214, 235274, 235284, 1208, 47555, 235265, 139, 236354, 238810, 239632, 236361, 697, 235288, 235323, 236179, 235248, 241293, 245670, 238080, 69781, 243156, 236648, 178008, 237354, 237707, 162200, 235336, 235248, 241305, 236392, 41388, 238693, 70754, 236669, 236375, 100280, 238513, 204551, 235265, 87997, 153728, 236511, 161050, 236137, 69781, 243156, 236137, 172829, 235832, 235248, 235274, 235323, 235832, 212026, 235269, 32275, 238810, 239632, 236361, 697, 235288, 235323, 236179, 235248, 241293, 245670, 238080, 69781, 243156, 236392, 49061, 227860, 237722, 32275, 238810, 239632, 236361, 697, 235280, 235323, 237807, 697, 235305, 235323, 236179, 235248, 241293, 245670, 238080, 69781, 243156, 236392, 235248, 235274, 235323, 22803, 235248, 245122, 238305, 179694, 235265, 11464, 236791, 51806, 236864, 238186, 26291, 235248, 245471, 22618, 55496, 235292, 697, 235295, 235278, 235288, 235275, 589, 235248, 235274, 728, 596, 235278, 235280, 235275, 728, 596, 235278, 235305, 14784, 596, 235278, 235280, 235275, 589, 730, 2552, 235282, 235274, 1214, 235304, 8160, 697, 235295, 235278, 235305, 235275, 589, 730, 2552, 235282, 235308, 1214, 235274, 235284, 1208, 139978, 109241, 78183, 236464, 21167, 237214, 212026, 11464, 147342, 236392, 51806, 236864, 238186, 236179, 26801, 237707, 72494, 80289, 239937, 236817, 238080, 22618, 55496, 235265, 161050, 236214, 115049, 237233, 81673, 21743, 235292, 596, 235278, 235288, 235275, 589, 235248, 235274, 728, 730, 2552, 235282, 235274, 1214, 235304, 235270, 728, 136672, 235282, 235308, 1214, 235274, 235284, 235270, 589, 730, 2552, 235282, 235274, 235284, 1214, 235274, 235284, 235270, 728, 136672, 235282, 235310, 1214, 235274, 235284, 235270, 728, 136672, 235282, 235308, 1214, 235274, 235284, 235270, 589, 730, 2552, 235282, 235304, 1214, 235274, 235284, 7101, 70754, 236645, 237807, 70754, 237551, 236791, 697, 235304, 235323, 236375, 38585, 240265, 237722, 11464, 70754, 236669, 236791, 184434, 236666, 22618, 55496, 235292, 596, 235278, 235288, 235275, 589, 730, 2552, 235282, 235274, 1214, 235310, 1208, 47555, 235265, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:12.354689Z",
     "start_time": "2024-04-03T05:13:12.351778Z"
    }
   },
   "id": "a71461b0df690398",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference before Fine-Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1cc5548c03cc80"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Chat Template\n",
    "def generate_response(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt }]\n",
    "    chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer.encode(chat, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=input_ids.to(model.device), max_new_tokens=max_new_tokens)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:12.361793Z",
     "start_time": "2024-04-03T05:13:12.355654Z"
    }
   },
   "id": "926796ff0f85510b",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#prompt = \"Write me a poem about Machine Learning.\"\n",
    "prompt = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:12.368986Z",
     "start_time": "2024-04-03T05:13:12.362669Z"
    }
   },
   "id": "99e67d0533bf0f91",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "머신러닝에 대한 시를 써주세요.\n",
      "model\n",
      "**머신러닝의 시**\n",
      "\n",
      "데이터가 쌓여진 바늘,\n",
      "모형이 학습해 숨어진 규칙.\n",
      "알고리즘이 작동하여,\n",
      "정보를 추출하고 패턴을 발견.\n",
      "\n",
      "인공지식이라는 마법,\n",
      "데이터를 분석하고 문제 해결.\n",
      "가벼운 망설이를 혐의,\n",
      "새로운 가능성을 열어 갑니다.\n",
      "\n",
      "사회를 바꾸는 기술,\n",
      "정보의 전달에 영향을 미친다.\n",
      "진료, 예측, 의료,\n",
      "머신러닝은 모든 분야에 영향을 미친다.\n",
      "\n",
      "따라서 머신러닝에 대한 희생감,\n",
      "지식의 발달에 대한 기대.\n",
      "미래를 위한 강력한 도움,\n",
      "세상을 더 나은 곳으로 바꾸는 힘.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:18.964386Z",
     "start_time": "2024-04-03T05:13:12.369903Z"
    }
   },
   "id": "acf71f317ede56b2",
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c994978d9d4dac94"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#text = \"Write me a poem about Machine Learning\"\n",
    "text = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:18.967840Z",
     "start_time": "2024-04-03T05:13:18.965410Z"
    }
   },
   "id": "1691f45038bc824b",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, summarization, and more. \n",
    "\n",
    "Please respond to the following text delimited by triple backticks in Korean.\n",
    "'''{text}'''\n",
    "Use Korean only.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:18.978136Z",
     "start_time": "2024-04-03T05:13:18.968783Z"
    }
   },
   "id": "cd578a38ddcd2fe0",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, summarization, and more. \n",
      "\n",
      "Please respond to the following text delimited by triple backticks in Korean.\n",
      "'''머신러닝에 대한 시를 써주세요.'''\n",
      "Use Korean only.\n",
      "model\n",
      "** Waktaverse-Gemma의 응답:**\n",
      "\n",
      "머신 러닝에 대한 시를 써드립니다.\n",
      "소박한 알고리즘,\n",
      "데이터를 학습합니다.\n",
      "모든 사물을 예측합니다.\n",
      "세상을 변화시키는 강력한력.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:21.143127Z",
     "start_time": "2024-04-03T05:13:18.979124Z"
    }
   },
   "id": "4ad69cbe1c67a913",
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supervised Fine-Tuning(LoRA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b43eba6780d2e557"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    text = (f\"instruction: {example['instruction'][0]}\\n\"\n",
    "            f\"output: {example['output'][0]}\")\n",
    "    return [text]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:21.146468Z",
     "start_time": "2024-04-03T05:13:21.143984Z"
    }
   },
   "id": "3c819cbc6c554745",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    formatting_func=formatting_func\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:13:21.652537Z",
     "start_time": "2024-04-03T05:13:21.147367Z"
    }
   },
   "id": "c60c5aa9a7d970b3",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='352' max='352' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [352/352 23:01, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>6.281600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>5.714500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>5.023100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.837300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.677200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.878400</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>3.317400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.876700</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.845400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.293800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>2.130500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.486800</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.957100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.309100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.480900</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.077400</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>2.291000</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.018400</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.824100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.197000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.898900</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>2.109700</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>2.008200</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>2.238700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.809700</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>2.075800</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.930900</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>2.044200</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.626200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.898100</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.882400</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>2.075900</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.598100</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>2.069300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.760500</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=352, training_loss=2.614435675469312, metrics={'train_runtime': 1381.4238, 'train_samples_per_second': 0.255, 'train_steps_per_second': 0.255, 'total_flos': 5199232443371520.0, 'train_loss': 2.614435675469312, 'epoch': 2.0})"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:36:23.256972Z",
     "start_time": "2024-04-03T05:13:21.653633Z"
    }
   },
   "id": "5d9ba03835392e2a",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trainer.save_model(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:36:24.295228Z",
     "start_time": "2024-04-03T05:36:23.257898Z"
    }
   },
   "id": "f4b2fdcf617359a4",
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference after Fine-Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ec24bf55b44834d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#text = \"Write me a poem about Machine Learning\"\n",
    "text = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:36:24.298537Z",
     "start_time": "2024-04-03T05:36:24.296183Z"
    }
   },
   "id": "31f9068a9634751a",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, summarization, and more. \n",
    "\n",
    "Please respond to the following text delimited by triple backticks in Korean.\n",
    "'''{text}'''\n",
    "Use Korean only.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:36:24.305299Z",
     "start_time": "2024-04-03T05:36:24.299383Z"
    }
   },
   "id": "bcba8c32d5f5469f",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "You are Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, summarization, and more. \n",
      "\n",
      "Please respond to the following text delimited by triple backticks in Korean.\n",
      "'''머신러닝에 대한 시를 써주세요.'''\n",
      "Use Korean only.\n",
      "model\n",
      "머신러닝은 컴퓨터 과학의 한 분야로, 인공지식을 학습하는 능력을 가진 컴퓨터 프로그램입니다. 텍스트, 이미지, 음악, 영상 등 다양한 형태의 데이터를 분석하고 학습하여 새로운 정보를 생성하는 능력을 가집니다. 머신러닝은 인공지식의 한 분야로, 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을 축적하는 과정을 컴퓨터가 수행하는 것을 목표로 합니다. 머신러닝은 인간이 지식을\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:38:37.479373Z",
     "start_time": "2024-04-03T05:36:24.307528Z"
    }
   },
   "id": "1219dfd08fc2facd",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae7bc882cc302767"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17eb26ba8e3342d6b087416297b2e6df"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_name)\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:39:02.477138Z",
     "start_time": "2024-04-03T05:38:37.480444Z"
    }
   },
   "id": "57bcdb4c1c233116",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T05:39:03.546524Z",
     "start_time": "2024-04-03T05:39:02.478767Z"
    }
   },
   "id": "db819b259928e006",
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.24k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85b1230e8b384c8bab7d957c83a111e9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf9171f701ce47e5877253d5d153fd1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24f818dd9c594d98bb0c37962bf2f867"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "26a49e617e8848c1b3a11665b225d8ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62f0646350dd469f8ff9b130b6ce0b71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ba3709ba8d54aaba6a4c07205144bc3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/5.24k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f5da5471e17447b9896d49d78b603bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/PathFinderKR/waktaverse-gemma-ko-7b-it/commit/ea032b3f7d6546fd043981ebce7d2eafc014664d', commit_message='Upload tokenizer', commit_description='', oid='ea032b3f7d6546fd043981ebce7d2eafc014664d', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push model and tokenizer to Hugging Face Hub\n",
    "model.push_to_hub(\n",
    "    repo_id=repo_id,\n",
    "    use_temp_dir=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    repo_id=repo_id,\n",
    "    use_temp_dir=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:25.427825Z",
     "start_time": "2024-04-03T05:39:03.547604Z"
    }
   },
   "id": "43aa8d6999c41016",
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
