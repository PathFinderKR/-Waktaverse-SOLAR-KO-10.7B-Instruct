{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Login to Hugging Face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c2f9104252a4bb"
  },
  {
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=token, # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4eee58c68eb02d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"Waktaverse-Llama-3-KO-8B-Instruct\"  # ADD YOUR MODEL NAME HERE\n",
    "username = \"PathFinderKR\"  # ADD YOUR USERNAME HERE\n",
    "repo_id = f\"{username}/{model_name}\"  # repository id"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd4c26069d51a597",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Downloads"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78114e8f78ee19ec"
  },
  {
   "cell_type": "code",
   "source": [
    "#!pip install huggingface_hub\n",
    "#!pip install transformers\n",
    "#!pip install bitsandbytes\n",
    "#!pip install peft\n",
    "#!pip install trl\n",
    "#!pip install accelerate\n",
    "#!pip install datasets\n",
    "#!pip install scikit-learn\n",
    "#!pip install packaging\n",
    "#!pip install ninja\n",
    "#!pip install flash-attn --no-build-isolation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce6454f84604b9e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "685ac8c0e05ac872"
  },
  {
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# datasets\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8c108abcd576306",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e512d30c21d105c2"
  },
  {
   "cell_type": "code",
   "source": [
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdf3a2a4e0e31554",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ce31a12e172a64"
  },
  {
   "cell_type": "code",
   "source": [
    "# seed\n",
    "seed=42\n",
    "\n",
    "# Tokenizer arguments\n",
    "max_length=256\n",
    "padding=\"do_not_pad\"\n",
    "truncation=True\n",
    "\n",
    "# model arguments\n",
    "num_return_sequences=1\n",
    "early_stopping=True\n",
    "num_beams=3\n",
    "min_new_tokens=1\n",
    "max_new_tokens=512\n",
    "do_sample=True\n",
    "temperature=0.6\n",
    "top_k=40\n",
    "top_p=0.9\n",
    "repetition_penalty=1.1\n",
    "\n",
    "# validation split\n",
    "validation_size=0.1\n",
    "\n",
    "# mixed precision\n",
    "dtype=torch.bfloat16\n",
    "\n",
    "# quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# SFTTrainer arguments\n",
    "max_seq_length=512"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc1a272a1c0b7ce4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5d3b7c47ea29a3d"
  },
  {
   "cell_type": "code",
   "source": [
    "# Model List\n",
    "\n",
    "# gemma variants\n",
    "# \"google/gemma-1.1-7b-it\"\n",
    "# \"google/codegemma-7b-it\"\n",
    "\n",
    "# llama2 variants\n",
    "# \"meta-llama/Meta-Llama-3-8B-Instruct\" // downloaded\n",
    "# \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "# \"PathFinderKR/Waktaverse-Llama-3-KO-8B-Instruct\"\n",
    "\n",
    "# mistral variants\n",
    "# \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# solar variants\n",
    "# \"upstage/SOLAR-10.7B-Instruct-v1.0\" // downloaded\n",
    "# \"PathFinderKR/Waktaverse-SOLAR-KO-10.7B-Instruct\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70d2fbe5c0980a54",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"",
   "metadata": {
    "collapsed": false
   },
   "id": "eb8160d8919cdaeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "special_tokens_dict = {'pad_token': '<|pad|>'}\n",
    "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "tokenizer.pad_token = '<|pad|>'\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ebbf883fce9aa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "978055543828a014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# display the model architecture\n",
    "display(Markdown(f'```{model}```'))"
   ],
   "id": "e224c50f423698e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4042d4b7cd2d0fd"
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"MarkrAI/KoCommercial-Dataset\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4373ef1c6a85e5e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# dataset information\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aabd9487c235d581",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# dataset example\n",
    "dataset[\"train\"][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "760399232efac9ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "6e54c42138a256a8"
  },
  {
   "cell_type": "code",
   "source": [
    "# Alpaca dataset format\n",
    "def preprocess_function(examples):\n",
    "    instruction = examples[\"instruction\"]\n",
    "    input_text = examples[\"input\"]\n",
    "    output_text = examples[\"output\"]\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": input_text,\n",
    "        \"output\": output_text\n",
    "    }\n",
    "    \n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f8905546b688101",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the dataset into a training and a validation dataset\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=validation_size, seed=seed)\n",
    "\n",
    "# Number of questions in the train, validation dataset\n",
    "print(f\"Number of questions in the train dataset: {len(dataset['train'])}\")\n",
    "print(f\"Number of questions in the validation dataset: {len(dataset['test'])}\")"
   ],
   "id": "e78ed83f07034d92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dataset examples\n",
    "print(dataset[\"train\"][0][\"instruction\"])\n",
    "print(dataset[\"train\"][0][\"input\"])\n",
    "print(dataset[\"train\"][0][\"output\"])"
   ],
   "id": "4cf2c30835e34a13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(dataset[\"test\"][0][\"instruction\"])\n",
    "print(dataset[\"test\"][0][\"input\"])\n",
    "print(dataset[\"test\"][0][\"output\"])"
   ],
   "id": "64a375461a6190c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference before Fine-Tuning",
   "id": "7e1cc5548c03cc80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_response(system ,user):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=False)\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        eos_token_id=tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        early_stopping=early_stopping,\n",
    "        num_beams=num_beams,\n",
    "        min_new_tokens=min_new_tokens,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ],
   "id": "926796ff0f85510b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_prompt = \"You are a poet. Write a poem about the following topic. Use Korean Only.\"",
   "id": "6296a3ab5e72f67b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#user_prompt = \"Write me a poem about Machine Learning.\"\n",
    "user_prompt = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99e67d0533bf0f91",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = generate_response(system_prompt, user_prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acf71f317ede56b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Supervised Fine-Tuning (LoRA)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b43eba6780d2e557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def formatting_func(example):\n",
    "    texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        instruction = example['instruction'][i]\n",
    "        input_text = example['input'][i]\n",
    "        output_text = example['output'][i]\n",
    "        text = (\n",
    "            f\"### Instruction: {instruction}\\n\"\n",
    "            f\"### Input: {input_text}\\n\"\n",
    "            f\"### Output: {output_text}\\n\"\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts"
   ],
   "id": "39f4d0e17c42f13d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response_template = \"### Output:\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(response_template=response_template, tokenizer=tokenizer)"
   ],
   "id": "6bf404f160aba0ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    formatting_func=formatting_func,\n",
    "    data_collator=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c60c5aa9a7d970b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5d9ba03835392e2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.save_model(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f4b2fdcf617359a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference after Fine-Tuning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ec24bf55b44834d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "system_prompt = \"You are a poet. Write a poem about the following topic. Use Korean Only.\"",
   "id": "d35aa851e8176d2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#user_prompt = \"Write me a poem about Machine Learning.\"\n",
    "user_prompt = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bcba8c32d5f5469f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "response = generate_response(system_prompt, user_prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1219dfd08fc2facd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Upload Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae7bc882cc302767"
  },
  {
   "cell_type": "code",
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, model_name)\n",
    "model = model.merge_and_unload()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "57bcdb4c1c233116",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "db819b259928e006",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Push model and tokenizer to Hugging Face Hub\n",
    "model.push_to_hub(\n",
    "    repo_id=repo_id,\n",
    "    use_temp_dir=False\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "    repo_id=repo_id,\n",
    "    use_temp_dir=False\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "43aa8d6999c41016",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
