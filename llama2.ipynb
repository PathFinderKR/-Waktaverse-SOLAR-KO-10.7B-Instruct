{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries",
   "id": "c8f33a502600bec9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:42.224096Z",
     "start_time": "2024-05-29T12:17:39.080797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# wandb\n",
    "import wandb"
   ],
   "id": "6c3dab257ca35b13",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Login to Hugging Face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c2f9104252a4bb"
  },
  {
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=token,  # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:42.554028Z",
     "start_time": "2024-05-29T12:17:42.225521Z"
    }
   },
   "id": "4eee58c68eb02d85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/pathfinder/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "model_name = \"Waktaverse-Llama-2-KO-7B-Instruct\"  # ADD YOUR MODEL NAME HERE\n",
    "username = \"PathFinderKR\"  # ADD YOUR USERNAME HERE\n",
    "repo_id = f\"{username}/{model_name}\"  # repository id"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:42.557394Z",
     "start_time": "2024-05-29T12:17:42.554924Z"
    }
   },
   "id": "cd4c26069d51a597",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Login to Weights & Biases",
   "id": "45b93503027bc62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.065022Z",
     "start_time": "2024-05-29T12:17:42.558284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(\n",
    "    key=api_key  # ADD YOUR API KEY HERE\n",
    ")\n",
    "wandb.init(project=model_name)"
   ],
   "id": "bd9885bdc6270f76",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /home/pathfinder/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/mnt/c/Users/cheir/GitHub/Waktaverse-LLM/wandb/run-20240529_211744-oueb21yv</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct/runs/oueb21yv' target=\"_blank\">cool-night-1</a></strong> to <a href='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct' target=\"_blank\">https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct/runs/oueb21yv' target=\"_blank\">https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct/runs/oueb21yv</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct/runs/oueb21yv?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f863b8fb310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e512d30c21d105c2"
  },
  {
   "cell_type": "code",
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.070170Z",
     "start_time": "2024-05-29T12:17:47.066841Z"
    }
   },
   "id": "fdf3a2a4e0e31554",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.078589Z",
     "start_time": "2024-05-29T12:17:47.071128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flash Attention Implementation\n",
    "if device == \"cuda:0\":\n",
    "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere, Ada, or Hopper GPUs\n",
    "        attn_implementation = \"flash_attention_2\"\n",
    "        torch_dtype = torch.bfloat16\n",
    "    else:\n",
    "        attn_implementation = \"eager\"\n",
    "        torch_dtype = torch.float16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float32\n",
    "print(f\"Attention Implementation = {attn_implementation}\")"
   ],
   "id": "d96a4ad9abc2d0ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Implementation = flash_attention_2\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84ce31a12e172a64"
  },
  {
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# Tokenizer parameters\n",
    "################################################################################\n",
    "max_length=4096\n",
    "padding=\"do_not_pad\"  # \"max_length\", \"longest\", \"do_not_pad\"\n",
    "truncation=True\n",
    "\n",
    "################################################################################\n",
    "# Generation parameters\n",
    "################################################################################\n",
    "num_return_sequences=1\n",
    "max_new_tokens=1024\n",
    "do_sample=True  # True for sampling, False for greedy decoding\n",
    "temperature=0.6\n",
    "top_p=0.9\n",
    "repetition_penalty=1.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "load_in_4bit=True\n",
    "bnb_4bit_compute_dtype=torch_dtype\n",
    "bnb_4bit_quant_type=\"nf4\"  # \"nf4\", #fp4\"\n",
    "bnb_4bit_use_double_quant=True\n",
    "\n",
    "################################################################################\n",
    "# LoRA parameters\n",
    "################################################################################\n",
    "task_type=\"CAUSAL_LM\"\n",
    "target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "r=16\n",
    "lora_alpha=32\n",
    "lora_dropout=0.1\n",
    "bias=\"none\"\n",
    "\n",
    "################################################################################\n",
    "# Training parameters\n",
    "################################################################################\n",
    "output_dir=\"./results\"\n",
    "logging_dir=\"./logs\"\n",
    "save_strategy=\"epoch\"\n",
    "logging_strategy=\"steps\"  # \"steps\", \"epoch\"\n",
    "if logging_strategy == \"steps\":\n",
    "    logging_steps=10\n",
    "else:\n",
    "    logging_steps=None\n",
    "save_total_limit=1\n",
    "report_to=\"wandb\"\n",
    "\n",
    "num_train_epochs=1\n",
    "per_device_train_batch_size=2\n",
    "gradient_accumulation_steps=2\n",
    "gradient_checkpointing=True\n",
    "bf16=True\n",
    "learning_rate=2e-5\n",
    "lr_scheduler_type=\"cosine\"  # \"constant\", \"linear\", \"cosine\"\n",
    "warmup_ratio=0.1\n",
    "optim = \"paged_adamw_32bit\"\n",
    "weight_decay=0.1\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "max_seq_length=4096\n",
    "packing=True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.084414Z",
     "start_time": "2024-05-29T12:17:47.079730Z"
    }
   },
   "id": "dc1a272a1c0b7ce4",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "516ea451de4ded55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.092516Z",
     "start_time": "2024-05-29T12:17:47.085427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Korean Tokenizer ID\n",
    "tokenizer_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ],
   "id": "5c5b21448100815d",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.354440Z",
     "start_time": "2024-05-29T12:17:47.093671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)"
   ],
   "id": "c50c6ca52d903d8d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.358085Z",
     "start_time": "2024-05-29T12:17:47.355582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "d27e6237718edd67",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.368055Z",
     "start_time": "2024-05-29T12:17:47.358945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Vocabulary size\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "# Special tokens\n",
    "print(f\"Special tokens: {tokenizer.special_tokens_map}\")\n",
    "# Padding side\n",
    "print(f\"Padding side: {tokenizer.padding_side}\")"
   ],
   "id": "59ccccfd53102df8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 32000\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}\n",
      "Padding side: right\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.376762Z",
     "start_time": "2024-05-29T12:17:47.368861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the tokenizer configuration\n",
    "display(Markdown(f'```{tokenizer}```'))"
   ],
   "id": "b67c01467026b872",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "```LlamaTokenizerFast(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n}```"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "fb5c1161ebd9df8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.381566Z",
     "start_time": "2024-05-29T12:17:47.377678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model ID for base model\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\""
   ],
   "id": "eb8160d8919cdaeb",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:17:47.389303Z",
     "start_time": "2024-05-29T12:17:47.383137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant\n",
    ")"
   ],
   "id": "69d30da42bd6cf7b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:03.150300Z",
     "start_time": "2024-05-29T12:17:47.390315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ],
   "id": "978055543828a014",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0910de92c5e642e689e97d9881983cb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:03.158861Z",
     "start_time": "2024-05-29T12:18:03.151595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the model architecture\n",
    "display(Markdown(f'```{model}```'))"
   ],
   "id": "e224c50f423698e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "```LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaFlashAttention2(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)```"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:03.174566Z",
     "start_time": "2024-05-29T12:18:03.159887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Number of parameters\n",
    "print(f\"Number of parameters (in billions): {model.num_parameters() / 1e9:.2f}\")"
   ],
   "id": "8cee59fa37c5f0bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters (in billions): 6.74\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "a4042d4b7cd2d0fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:03.183234Z",
     "start_time": "2024-05-29T12:18:03.175595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset ID\n",
    "dataset_id = \"beomi/KoAlpaca-v1.1a\""
   ],
   "id": "4cd4661dab93e4c8",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(dataset_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.008859Z",
     "start_time": "2024-05-29T12:18:03.184320Z"
    }
   },
   "id": "4373ef1c6a85e5e2",
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# Dataset information\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.013309Z",
     "start_time": "2024-05-29T12:18:07.009909Z"
    }
   },
   "id": "aabd9487c235d581",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'url'],\n",
       "        num_rows: 21155\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.022273Z",
     "start_time": "2024-05-29T12:18:07.014204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset example\n",
    "print(dataset[\"train\"][0][\"instruction\"])\n",
    "print(dataset[\"train\"][0][\"output\"])"
   ],
   "id": "4cf2c30835e34a13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "양파는 어떤 식물 부위인가요? 그리고 고구마는 뿌리인가요?\n",
      "양파는 잎이 아닌 식물의 줄기 부분입니다. 고구마는 식물의 뿌리 부분입니다. \n",
      "\n",
      "식물의 부위의 구분에 대해 궁금해하는 분이라면 분명 이 질문에 대한 답을 찾고 있을 것입니다. 양파는 잎이 아닌 줄기 부분입니다. 고구마는 다른 질문과 답변에서 언급된 것과 같이 뿌리 부분입니다. 따라서, 양파는 식물의 줄기 부분이 되고, 고구마는 식물의 뿌리 부분입니다.\n",
      "\n",
      " 덧붙이는 답변: 고구마 줄기도 볶아먹을 수 있나요? \n",
      "\n",
      "고구마 줄기도 식용으로 볶아먹을 수 있습니다. 하지만 줄기 뿐만 아니라, 잎, 씨, 뿌리까지 모든 부위가 식용으로 활용되기도 합니다. 다만, 한국에서는 일반적으로 뿌리 부분인 고구마를 주로 먹습니다.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "6e54c42138a256a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.033037Z",
     "start_time": "2024-05-29T12:18:07.023144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)"
   ],
   "id": "c177f01dab4ce2a0",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.884236Z",
     "start_time": "2024-05-29T12:18:07.034114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Alpaca dataset format: \n",
    "# {\"instruction\": [str],\n",
    "#   \"input\": [str],\n",
    "#   \"output\": [str]}\n",
    "\n",
    "# Korean\n",
    "def prompt_without_input(example):\n",
    "    text = (\n",
    "        \"<s>[INST]<<SYS>>\\n\"\n",
    "        \"다음은 작업을 설명하는 지시사항입니다. 요청을 적절하게 완료하는 응답을 작성하세요.\\n\"\n",
    "        \"<</SYS>>\\n\\n\"\n",
    "\n",
    "        f\"{example['instruction']}[/INST]{example['output']}</s>\"\n",
    "        )\n",
    "    return {'text': text}\n",
    "\n",
    "# Apply the alpaca prompt to the dataset\n",
    "dataset = dataset.map(prompt_without_input)"
   ],
   "id": "fd3aaf9eac48027a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/21155 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f996ee4ee1cf40bcbaa8e0bfc723415f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.888687Z",
     "start_time": "2024-05-29T12:18:07.885312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the first example\n",
    "print(dataset[\"train\"][0][\"text\"])"
   ],
   "id": "2161788a1129971e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST]<<SYS>>\n",
      "다음은 작업을 설명하는 지시사항입니다. 요청을 적절하게 완료하는 응답을 작성하세요.\n",
      "<</SYS>>\n",
      "\n",
      "케첩과 마요네즈의 입구 모양은 왜 다른가요? \n",
      "보통 케찹과 마요네즈는 한 세트자나요? 그런데 보면 케찹은 원터치 뚜껑에다가 새 케찹은 뚜껑을 돌려서 열면 얇은 알미늄 같은 막도 이꾸또 케찹이 나오는 구멍이 동그랗자나요? 근데 마요네즈는 돌리는 뚜껑에다가 처음에 새로 사도 알미늄 같은 막두 엄꾸나오는 구멍 모양도 별 모양인데 왜 그런가요?[/INST]케첩과 마요네즈의 입구 모양이 다른 것은 끈적이는 정도 즉, 물질의 점성(Viscosity) 차이 때문입니다. 마요네즈는 케첩보다 점성이 높기 때문에 돌리는 뚜껑에 별 모양의 입구를 만들어도 내부 액체의 모양이 유지됩니다. 하지만 케첩은 끈적거림이 적기 때문에 별 모양의 입구를 만들어도 케첩 내부의 액체가 곧 둥글게 뭉쳐지기 때문에 케첩의 뚜껑은 동그란 구멍입니다. 따라서 입구 모양은 물질의 점성(Viscosity) 차이 때문입니다.</s>\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Supervised Fine-Tuning (LoRA)",
   "id": "b43eba6780d2e557"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.967374Z",
     "start_time": "2024-05-29T12:18:07.889630Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare model for kbit training\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "84c74906e328d032",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:07.971445Z",
     "start_time": "2024-05-29T12:18:07.968452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = LoraConfig(\n",
    "    task_type=task_type,\n",
    "    target_modules=target_modules,\n",
    "    r=r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=bias\n",
    ")"
   ],
   "id": "75ef555547f56ebf",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:08.522484Z",
     "start_time": "2024-05-29T12:18:07.972296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Number of trainable parameters\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ],
   "id": "e4d2d7975a1ff14e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 39,976,960 || all params: 6,778,392,576 || trainable%: 0.5898\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:08.606127Z",
     "start_time": "2024-05-29T12:18:08.523413Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=logging_dir,\n",
    "    save_strategy=save_strategy,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    report_to=report_to,\n",
    "    \n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    bf16=bf16,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    optim=optim,\n",
    "    weight_decay=weight_decay\n",
    ")"
   ],
   "id": "2e29a9a3ca3b96b1",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=packing\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-29T12:18:16.282075Z",
     "start_time": "2024-05-29T12:18:08.607239Z"
    }
   },
   "id": "c60c5aa9a7d970b3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b500436e5b5d47318eb0d8de4923a209"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": "trainer.train()",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:14:59.982312Z",
     "start_time": "2024-05-29T12:18:16.283338Z"
    }
   },
   "id": "5d9ba03835392e2a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='946' max='946' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [946/946 4:56:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.331900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.225900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.989300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.939000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.915600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.907200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.887000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.891200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.851800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.829300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.841200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.791700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.782700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.776400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.769200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.776200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.763900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.784600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.770200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.768100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.778400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.768800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.778700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.781400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.752500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.748500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.763600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.754400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.749100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.764300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.750800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.750600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.746400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.752400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.763000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.757900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.746800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.752300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.765800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.755300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.768500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.745400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=946, training_loss=0.8208524938869678, metrics={'train_runtime': 17803.4844, 'train_samples_per_second': 0.213, 'train_steps_per_second': 0.053, 'total_flos': 6.181714590003364e+17, 'train_loss': 0.8208524938869678, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.finish()\n",
    "trainer.save_model(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:15:10.051793Z",
     "start_time": "2024-05-29T17:14:59.985446Z"
    }
   },
   "id": "f4b2fdcf617359a4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='152.571 MB of 152.571 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88659bc310994f828e56a3f1d9ff108d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>██▆▂▁▁▁▁▂▃▃▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>train/learning_rate</td><td>▂▃▅▇███████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>6.181714590003364e+17</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>946</td></tr><tr><td>train/grad_norm</td><td>0.32686</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.7454</td></tr><tr><td>train_loss</td><td>0.82085</td></tr><tr><td>train_runtime</td><td>17803.4844</td></tr><tr><td>train_samples_per_second</td><td>0.213</td></tr><tr><td>train_steps_per_second</td><td>0.053</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-night-1</strong> at: <a href='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct/runs/oueb21yv' target=\"_blank\">https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct/runs/oueb21yv</a><br/> View project at: <a href='https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct' target=\"_blank\">https://wandb.ai/pathfinderkr/Waktaverse-Llama-2-KO-7B-Instruct</a><br/>Synced 6 W&B file(s), 0 media file(s), 6 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240529_211744-oueb21yv/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": "## Inference",
   "metadata": {
    "collapsed": false
   },
   "id": "4ec24bf55b44834d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:15:10.055857Z",
     "start_time": "2024-05-29T17:15:10.052885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompt_template(system, user):\n",
    "    return (\n",
    "        \"<s>[INST]<<SYS>>\\n\"\n",
    "        f\"{system}\\n\"\n",
    "        \"<</SYS>>\\n\\n\"\n",
    "        \n",
    "        f\"{user}[/INST]\"\n",
    "    )"
   ],
   "id": "cd97cb5f0b719d5e",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:15:10.067290Z",
     "start_time": "2024-05-29T17:15:10.056925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_response(system ,user):\n",
    "    prompt = prompt_template(system, user)\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ],
   "id": "926796ff0f85510b",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T17:15:10.082610Z",
     "start_time": "2024-05-29T17:15:10.068203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#system_prompt = \"You are a helpful assistant. Respond to the following user prompt.\"\n",
    "system_prompt = \"다음 지시사항에 대한 응답을 작성해주세요.\"\n",
    "#user_prompt = \"Write me a poem about Machine Learning.\"\n",
    "user_prompt = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "id": "d35aa851e8176d2d",
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "response = generate_response(system_prompt, user_prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "end_time": "2024-05-29T17:16:16.987059Z",
     "start_time": "2024-05-29T17:15:10.083469Z"
    }
   },
   "id": "1219dfd08fc2facd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST]<<SYS>>\n",
      "다음 지시사항에 대한 응답을 작성해주세요.\n",
      "<</SYS>>\n",
      "\n",
      "머신러닝에 대한 시를 써주세요.[/INST]시:\n",
      "\n",
      "처음에는 어려웠지만, 계속해서 연습하면서 자연스럽게 배운 머신러닝이라는 말은 독일어의 \"Maschinen\"에서 유래된 단어입니다. 이 단어는 기계나 동물의 무거운 일을 불가능하게 하는 능력을 뜻하고, 따라서 머신러닝은 쉽게 풀리지 않는 문제를 풀 수 있는 기술을 말합니다. 이 말은 과학 분야에서도 사용되며, 복잡한 계산 방식을 빠르게 완료하는 기술을 말합니다.</s>\n"
     ]
    }
   ],
   "execution_count": 35
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
