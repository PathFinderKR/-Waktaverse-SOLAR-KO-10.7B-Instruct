{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Login to Hugging Face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98126af063c251e0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:29.320038Z",
     "start_time": "2024-04-02T06:04:28.578119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/pathfinder/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=token, # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f21a95a142b71e93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:31.531078Z",
     "start_time": "2024-04-02T06:04:29.333013Z"
    }
   },
   "id": "108874429856aabe",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f29fda50194a3fe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:31.690393Z",
     "start_time": "2024-04-02T06:04:31.532765Z"
    }
   },
   "id": "5555b5512cd1a1dd",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b452e9ab5ed33883"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenizer arguments\n",
    "max_length = 1024\n",
    "\n",
    "# model arguments\n",
    "max_new_tokens=500\n",
    "\n",
    "# mixed precision\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:31.706712Z",
     "start_time": "2024-04-02T06:04:31.698524Z"
    }
   },
   "id": "778930f2cc7c1224",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14f31dc8d02b7233"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Model List\n",
    "\n",
    "# gemma variants\n",
    "# \"google/gemma-7b-it\" // downloaded\n",
    "# \"PathFinderKR/waktaverse-gemma-ko-7b-it\"\n",
    "# \"beomi/gemma-ko-7b\"\n",
    "\n",
    "# llama2 variants\n",
    "# \"meta-llama/Llama-2-7b-chat-hf\" // downloaded\n",
    "# \"PathFinderKR/waktaverse-Llama-2-ko-7b-it\"\n",
    "# \"beomi/KoAlpaca-Polyglot-5.8B\" // downloaded\n",
    "# \"beomi/open-llama-2-ko-7b\"\n",
    "\n",
    "# solar variants\n",
    "# \"chihoonlee10/T3Q-ko-solar-dpo-v4.0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:31.711653Z",
     "start_time": "2024-04-02T06:04:31.707799Z"
    }
   },
   "id": "3a8abd326253137e",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_id = \"PathFinderKR/waktaverse-gemma-ko-7b-it\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T06:04:31.718704Z",
     "start_time": "2024-04-02T06:04:31.712651Z"
    }
   },
   "id": "352852804a6a38db",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pathfinder/anaconda3/envs/torch-env/lib/python3.11/site-packages/transformers/quantizers/auto.py:159: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "987c38913d3440378b7750194b630fa4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "60187e7ddfa144c094a7673356fea77a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-02T06:04:31.719832Z"
    }
   },
   "id": "f5f879cefa518232",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the model on a single GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b389c694ab4ab89f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Chat Template\n",
    "def generate_response(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer.encode(chat, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=input_ids.to(model.device), max_new_tokens=max_new_tokens)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c849786ee0282d8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#prompt = \"Write me a poem about Machine Learning.\"\n",
    "prompt = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e2c71d41fed35b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aac19ad24cb1cca2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39cf87b1c6a4e54d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#text = \"Write me a poem about Machine Learning\"\n",
    "text = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794f458504c5b05a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, translation, summarization, and more. \n",
    "\n",
    "Please respond to the following text delimited by triple backticks in Korean.\n",
    "Use Korean only.\n",
    "'''{text}'''\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fd4695d3ef5a9e8",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99c3c9bbedd489a3",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
