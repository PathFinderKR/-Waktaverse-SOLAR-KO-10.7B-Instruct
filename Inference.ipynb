{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Login to Hugging Face"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98126af063c251e0"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:30.955926Z",
     "start_time": "2024-04-03T06:06:30.359601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/pathfinder/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=token, # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f21a95a142b71e93"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:32.220526Z",
     "start_time": "2024-04-03T06:06:30.957247Z"
    }
   },
   "id": "108874429856aabe",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f29fda50194a3fe"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:32.391209Z",
     "start_time": "2024-04-03T06:06:32.221596Z"
    }
   },
   "id": "5555b5512cd1a1dd",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b452e9ab5ed33883"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Tokenizer arguments\n",
    "max_length = 1024\n",
    "\n",
    "# model arguments\n",
    "max_new_tokens=500\n",
    "\n",
    "# mixed precision\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "# quantization configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:32.400026Z",
     "start_time": "2024-04-03T06:06:32.392387Z"
    }
   },
   "id": "778930f2cc7c1224",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14f31dc8d02b7233"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Model List\n",
    "\n",
    "# gemma variants\n",
    "# \"google/gemma-7b-it\" // downloaded\n",
    "# \"PathFinderKR/waktaverse-gemma-ko-7b-it\"\n",
    "# \"beomi/gemma-ko-7b\"\n",
    "\n",
    "# llama2 variants\n",
    "# \"meta-llama/Llama-2-7b-chat-hf\" // downloaded\n",
    "# \"PathFinderKR/waktaverse-Llama-2-ko-7b-it\"\n",
    "# \"beomi/KoAlpaca-Polyglot-5.8B\" // downloaded\n",
    "# \"beomi/open-llama-2-ko-7b\"\n",
    "\n",
    "# solar variants\n",
    "# \"chihoonlee10/T3Q-ko-solar-dpo-v4.0\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:32.405078Z",
     "start_time": "2024-04-03T06:06:32.401505Z"
    }
   },
   "id": "3a8abd326253137e",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model_id = \"PathFinderKR/waktaverse-gemma-ko-7b-it\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:06:32.412493Z",
     "start_time": "2024-04-03T06:06:32.406101Z"
    }
   },
   "id": "352852804a6a38db",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f6c61c8cb444eb59dcc7bfbb16644ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3f511f8adca4a05a75d01579d413e81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/522 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "996b39d3462f4f2489731fb95bb97c04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aeaa6e5abcaf4c23ad0cef5d0f46ab92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b3df308f2e145b993656b6f54218490"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94f13090b1cf4ff999fb7c0d56467f9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7717919ec4114729b4b2f6bf469db11d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba7e8aa183c049de81c0f902368655e8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9ea442305b74cada830e4e8a6d23481"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29fec3ce36de4cceb56c3ff57ce1aeea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87f4ae4ee7014228b472fb55640d4fd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf8111dddcd943399a0dca7217b7d764"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:37.476173Z",
     "start_time": "2024-04-03T06:06:32.413947Z"
    }
   },
   "id": "f5f879cefa518232",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running the model on a single GPU"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b389c694ab4ab89f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Chat Template\n",
    "def generate_response(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    input_ids = tokenizer.encode(chat, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    outputs = model.generate(input_ids=input_ids.to(model.device), max_new_tokens=max_new_tokens)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:37.489596Z",
     "start_time": "2024-04-03T06:32:37.480999Z"
    }
   },
   "id": "1c849786ee0282d8",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#prompt = \"Write me a poem about Machine Learning.\"\n",
    "#prompt = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:37.506017Z",
     "start_time": "2024-04-03T06:32:37.490807Z"
    }
   },
   "id": "7e2c71d41fed35b0",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = \"코루틴에 대해 설명해줘\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:37.514426Z",
     "start_time": "2024-04-03T06:32:37.507356Z"
    }
   },
   "id": "e9e50b0e4eece0c2",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "코루틴에 대해 설명해줘\n",
      "model\n",
      "**코루틴**\n",
      "\n",
      "코루틴은 비동기 프로그래밍 개념의 하나입니다. 코루틴은 스스로가 동시에 실행되는 것 같지만, 실제는 스택을 사용하지 않고 대기 상태로 넘어갑니다. 코루틴은 다른 코루틴이 완료되거나 이벤트가 발생할 때까지 대기합니다.\n",
      "\n",
      "**주요 특징:**\n",
      "\n",
      "* **비동기:** 코루틴은 동시에 실행되지만, 대기 상태로 넘어갑니다.\n",
      "* **스택 없이:** 코루틴은 스택을 사용하지 않고 대기 상태로 넘어갑니다.\n",
      "* **순차적:** 코루틴은 순차적으로 실행됩니다.\n",
      "* **상태:** 코루틴은 상태를 유지하지 않고 대기 상태로 넘어갑니다.\n",
      "* **콜백:** 코루틴이 완료되거나 이벤트가 발생하면 콜백 함수가 호출됩니다.\n",
      "* **정합:** 코루틴은 다른 코루틴이 동시에 실행되는 것 같지만, 실제는 순차적으로 실행됩니다.\n",
      "\n",
      "**장점:**\n",
      "\n",
      "* **순차적:** 코루틴은 순차적으로 실행되므로 코드를 더 간단하게 작성할 수 있습니다.\n",
      "* **비동기:** 코루틴은 비동기 작업을 쉽게 처리할 수 있습니다.\n",
      "* **상태 없이:** 코루틴은 상태를 유지하지 않고 대기 상태로 넘어갑니다.\n",
      "* **콜백:** 코루틴이 완료되거나 이벤트가 발생하면 콜백 함수가 호출됩니다.\n",
      "\n",
      "**단점:**\n",
      "\n",
      "* **순차적 제한:** 코루틴은 순차적으로 실행되므로, 동시에 수행하는 작업에는 제한이 있습니다.\n",
      "* **대기:** 코루틴은 대기 상태로 넘어갑니다. 따라서 모든 코루틴이 완료되거나 이벤트가 발생하기 전까지 대기해야 합니다.\n",
      "* **사고:** 코루틴을 사용하는 데는 숙련이 필요합니다.\n",
      "\n",
      "**예제:**\n",
      "\n",
      "```python\n",
      "def hello(name):\n",
      "  print(\"Hello, \" + name)\n",
      "\n",
      "def main():\n",
      "  hello\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:55.550282Z",
     "start_time": "2024-04-03T06:32:37.515473Z"
    }
   },
   "id": "aac19ad24cb1cca2",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prompt Engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39cf87b1c6a4e54d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#text = \"Write me a poem about Machine Learning\"\n",
    "text = \"머신러닝에 대한 시를 써주세요.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:55.553546Z",
     "start_time": "2024-04-03T06:32:55.551343Z"
    }
   },
   "id": "794f458504c5b05a",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, translation, summarization, and more. \n",
    "\n",
    "Please respond to the following text delimited by triple backticks in Korean.\n",
    "'''{text}'''\n",
    "Use Korean only.\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:32:55.560645Z",
     "start_time": "2024-04-03T06:32:55.554392Z"
    }
   },
   "id": "2fd4695d3ef5a9e8",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user\n",
      "Waktaverse-Gemma, a Korean language model, capable of performing various natural language processing tasks such as text generation, question answering, translation, summarization, and more. \n",
      "\n",
      "Please respond to the following text delimited by triple backticks in Korean.\n",
      "'''머신러닝에 대한 시를 써주세요.'''\n",
      "Use Korean only.\n",
      "model\n",
      "머신 러닝, 컴퓨터 과학의 한 분야\n",
      "데이터를 학습하여 텍스트, 이미지, 음악, 영상 등 다양한 종류의 정보를 분석하고 생성하는 능력을 가집니다.\n",
      "지식을 축적하는 과정에서 머신 러닝은 인공지식의 한 부분으로 자 자유로운 학습 과정을 거쳐 컴퓨터 시스템의 모든 부분에서 인간 수준의 성능을 보이거나 능가하는 수준으로 발달합니다.\n",
      "머신 러닝은 인공지식의 한 부분으로, 인간 수준의 성능을 가진 컴퓨터 시스템을 만듭니다. 머신 러닝은 크게 두 가지 주요 분야로 구성됩니다. 첫 번째 분야는 학습된 모델을 사용하여 새로운 데이터를 예측하는 것입니다. 두 번째 분야는 학습된 모델을 사용하여 새로운 데이터를 예측하는 것입니다. 두 번째 분야는 학습된 모델을 사용하여 새로운 데이터를 예측하는 것입니다.\n",
      "머신 러닝은 인공지식 기술의 한 부분으로, 인간 수준의 성능을 가진 컴퓨터 시스템을 만듭니다. 머신 러닝은 크게 두 가지 주요 분야로 구성됩니다. 첫 번째 분야는 학습된 모델을 사용하여 새로운 데이터를 예측하는 것입니다. 두 번째 분야는 학습된 모델을 사용하여 새로운 데이터를 예측하는 것입니다. 두 번째 분야는 학습된 모델을 사용하여 새로운 데이터를 예측하는 것입니다.\n"
     ]
    }
   ],
   "source": [
    "response = generate_response(prompt)\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T06:33:06.441808Z",
     "start_time": "2024-04-03T06:32:55.563680Z"
    }
   },
   "id": "99c3c9bbedd489a3",
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
