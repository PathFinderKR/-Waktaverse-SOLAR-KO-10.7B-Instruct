{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Importing Libraries",
   "id": "27bfec669e938b1d"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:55.220500Z",
     "start_time": "2024-06-05T15:03:52.908845Z"
    }
   },
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import locale\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pacmap\n",
    "import plotly.express as px\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextStreamer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# cohere\n",
    "import cohere"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:55.224338Z",
     "start_time": "2024-06-05T15:03:55.221900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set locale to UTF-8\n",
    "locale.getpreferredencoding = lambda: 'UTF-8'\n",
    "# Set pandas display options\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "# # Set MKL_THREADING_LAYER to GNU\n",
    "os.environ['MKL_THREADING_LAYER']='GNU'"
   ],
   "id": "f16942b89ad53346",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Login to cohere",
   "id": "3495e9d6c61bb90d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "co = cohere.Client(api_key)"
   ],
   "id": "dbbadee545b9e17a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "5171e6716181402f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:55.460631Z",
     "start_time": "2024-06-05T15:03:55.225319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    \"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "id": "fb9bb42087150d27",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cuda:0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:55.489297Z",
     "start_time": "2024-06-05T15:03:55.462244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flash Attention Implementation\n",
    "if device == \"cuda:0\":\n",
    "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere, Ada, or Hopper GPUs\n",
    "        attn_implementation = \"flash_attention_2\"\n",
    "        torch_dtype = torch.bfloat16\n",
    "    else:\n",
    "        attn_implementation = \"eager\"\n",
    "        torch_dtype = torch.float16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float32\n",
    "print(f\"Attention Implementation = {attn_implementation}\")"
   ],
   "id": "7adc66431e878247",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Implementation = flash_attention_2\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters",
   "id": "80e1470d28f5e2f4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:55.493963Z",
     "start_time": "2024-06-05T15:03:55.490292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# Tokenizer parameters\n",
    "################################################################################\n",
    "max_length=8192\n",
    "padding=\"do_not_pad\"  # \"max_length\", \"longest\", \"do_not_pad\"\n",
    "truncation=True\n",
    "\n",
    "################################################################################\n",
    "# Generation parameters\n",
    "################################################################################\n",
    "num_return_sequences=1\n",
    "max_new_tokens=1024\n",
    "do_sample=True  # True for sampling, False for greedy decoding\n",
    "temperature=0.6\n",
    "top_p=0.9\n",
    "repetition_penalty=1.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "load_in_4bit=True\n",
    "bnb_4bit_compute_dtype=torch_dtype\n",
    "bnb_4bit_quant_type=\"nf4\"  # \"nf4\", #fp4\"\n",
    "bnb_4bit_use_double_quant=True\n",
    "\n",
    "################################################################################\n",
    "# Retriever parameters\n",
    "################################################################################\n",
    "k=5"
   ],
   "id": "fcb006a228a847d6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "819d4c4bc9b77e43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Model ID\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ],
   "id": "ece41d2cbe42738d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "streamer = TextStreamer(tokenizer)"
   ],
   "id": "76bd481bd0c8e896"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_use_double_quant=bnb_4bit_use_double_quant\n",
    ")"
   ],
   "id": "ffe932ffc616d0f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ],
   "id": "d62568ac3c5065b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:55.502097Z",
     "start_time": "2024-06-05T15:03:55.494926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Embedding model ID\n",
    "embedding_model_id = \"Cohere/Cohere-embed-multilingual-light-v3.0\""
   ],
   "id": "13aed6c184d3fc5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Reranking model ID\n",
    "reranking_model_id = \"rerank-multilingual-v3.0\""
   ],
   "id": "5fd080db8eb1c2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Documents",
   "id": "cfd5207181c6296d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:03:58.646399Z",
     "start_time": "2024-06-05T15:03:58.643939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Document ID\n",
    "document_id = \"Cohere/wikipedia-22-12-ko-embeddings\""
   ],
   "id": "42205bb2f025e781",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-05T15:04:21.220470Z",
     "start_time": "2024-06-05T15:03:58.647974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load documents\n",
    "documents = load_dataset(document_id,  split=\"train\")\n",
    "documents = torch.tensor(documents)"
   ],
   "id": "8526ba12b3e585f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/647897 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca9ed408582645208404b4e654a0f247"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## RAG",
   "id": "3843aebe696cf92c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prompt_template(context, question):\n",
    "    return (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n",
    "        \"You are Korean. Use Korean only. 한국어만 사용하세요.\\n\"\n",
    "        \"Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked. \\n\"\n",
    "        \"<|eot_id|>\"\n",
    "        \n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        \"###Context\\n\"\n",
    "        f\"{context}\\n\"\n",
    "        f\"###Question: {question}<|eot_id|>\"\n",
    "        \n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    )"
   ],
   "id": "e6bf24fd94c7b39a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_response(query):\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    query_embedding = co.embed(\n",
    "        texts=query, \n",
    "        model=embedding_model_id,\n",
    "        input_type='search_query',\n",
    "        embedding_types=['float']\n",
    "    ).embeddings\n",
    "    query_embedding = torch.tensor(query_embedding)\n",
    "    \n",
    "    dot_scores = torch.mm(query_embedding, documents.transpose(0, 1))\n",
    "    top_k = torch.topk(dot_scores, k=k)\n",
    "    \n",
    "    retrieved_docs = []\n",
    "    for doc_id in top_k.indices[0].tolist():\n",
    "        retrieved_docs.append(documents[doc_id])\n",
    "\n",
    "    print(\"=> Generating response...\")\n",
    "    prompt = prompt_template(retrieved_docs, query)\n",
    "    \n",
    "    input_ids = tokenizer.encode(\n",
    "        prompt,\n",
    "        max_length=max_length,\n",
    "        padding=padding,\n",
    "        truncation=truncation,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ],
   "id": "1b5b0424db0bffbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "user_prompt = \"한국의 대통령은 누구인가요?\"",
   "id": "3bcd9fb24840dedc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Retrieval for {user_prompt}...\")\n",
    "print(\"\\n==================================Top document==================================\")\n",
    "query_embedding = co.embed(\n",
    "    texts=user_prompt, \n",
    "    model=embedding_model_id,\n",
    "    input_type='search_query',\n",
    "    embedding_types=['float']\n",
    ").embeddings\n",
    "query_embedding = torch.tensor(query_embedding)\n",
    "\n",
    "dot_scores = torch.mm(query_embedding, documents.transpose(0, 1))\n",
    "top_k = torch.topk(dot_scores, k=k)\n",
    "\n",
    "docs = []\n",
    "for doc_id in top_k.indices[0].tolist():\n",
    "    docs.append(documents[doc_id])\n",
    "    print(\"====================================================================\")\n",
    "print(\"==================================Metadata==================================\")\n",
    "for k in range(len(docs)):\n",
    "    print(f\"Document {k+1}:\")\n",
    "    print(docs[k].metadata)\n",
    "print(\"====================================================================\\n\")"
   ],
   "id": "ab45538999a47cc3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "response = generate_response(user_prompt)",
   "id": "8093ac9eb977ace7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
