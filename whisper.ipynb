{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Login to Hugging Face",
   "id": "ba734f4553b55bde"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:05.626409Z",
     "start_time": "2024-05-17T03:00:05.516749Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(\n",
    "    token=token,  # ADD YOUR TOKEN HERE\n",
    "    add_to_git_credential=True\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a54b681f3f1f413e8481dba2842baf01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "74f9c395dc73b9ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_name = \"Waktaverse-whisper-KO-large-v3\"  # ADD YOUR MODEL NAME HERE\n",
    "username = \"PathFinderKR\"  # ADD YOUR USERNAME HERE\n",
    "repo_id = f\"{username}/{model_name}\"  # repository id"
   ],
   "id": "10278940e0efdeef",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Login to Weights & Biases",
   "id": "bda17620db56f7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:10.800351Z",
     "start_time": "2024-05-17T03:00:05.632719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "\n",
    "api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "wandb.login(\n",
    "    key=api_key  # ADD YOUR API KEY HERE\n",
    ")\n",
    "wandb.init(project=model_name)"
   ],
   "id": "4bd928034393af5c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mpathfinderkr\u001B[0m (\u001B[33mwaktaverse\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/pathfinder/Documents/GitHub/Waktaverse-LLM/wandb/run-20240517_120007-cx1bc1fh</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3/runs/cx1bc1fh' target=\"_blank\">prime-sun-17</a></strong> to <a href='https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3' target=\"_blank\">https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3/runs/cx1bc1fh' target=\"_blank\">https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3/runs/cx1bc1fh</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/waktaverse/Waktaverse-whisper-KO-large-v3/runs/cx1bc1fh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x112068f90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "ac75bbafc649a044"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:13.267057Z",
     "start_time": "2024-05-17T03:00:10.801431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "\n",
    "# huggingface\n",
    "from transformers import (\n",
    "    AutoModelForSpeechSeq2Seq,\n",
    "    AutoProcessor,\n",
    "    pipeline,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# datasets\n",
    "from datasets import Audio, load_dataset, DatasetDict"
   ],
   "id": "fbbf7dc485d2e81d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "64b0506e05ceda89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:13.270442Z",
     "start_time": "2024-05-17T03:00:13.267743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device setup\n",
    "device = (\n",
    "    \"cuda:0\" if torch.cuda.is_available() else # Nvidia GPU\n",
    "    #\"mps\" if torch.backends.mps.is_available() else # Apple Silicon GPU\n",
    "    \"cpu\"\n",
    ")\n",
    "print(f\"Device = {device}\")"
   ],
   "id": "d9854d2cc03800ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:13.274639Z",
     "start_time": "2024-05-17T03:00:13.270968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flash Attention Implementation\n",
    "if device == \"cuda:0\":\n",
    "    if torch.cuda.get_device_capability()[0] >= 8: # Ampere, Ada, or Hopper GPUs\n",
    "        attn_implementation = \"flash_attention_2\"\n",
    "        torch_dtype = torch.float16\n",
    "    else:\n",
    "        attn_implementation = \"eager\"\n",
    "        torch_dtype = torch.float16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float32\n",
    "print(f\"Attention Implementation = {attn_implementation}\")"
   ],
   "id": "71e388c237d897cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Implementation = eager\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameters",
   "id": "e5738bbdd3f5609a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:13.282928Z",
     "start_time": "2024-05-17T03:00:13.275234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# Language\n",
    "################################################################################\n",
    "language = \"Korean\"\n",
    "language_code = \"ko\"\n",
    "\n",
    "################################################################################\n",
    "# seed\n",
    "################################################################################\n",
    "seed=42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "################################################################################\n",
    "# Generation parameters\n",
    "################################################################################\n",
    "max_new_tokens=100\n",
    "chunk_length_s=30\n",
    "batch_size=4\n",
    "sampling_rate=16000\n",
    "\n",
    "################################################################################\n",
    "# Training parameters\n",
    "################################################################################\n",
    "output_dir=\"./results\"\n",
    "logging_dir=\"./logs\"\n",
    "save_strategy=\"epoch\"\n",
    "logging_strategy=\"steps\" # \"steps\", \"epoch\"\n",
    "if logging_strategy == \"steps\":\n",
    "    logging_steps=10\n",
    "else:\n",
    "    logging_steps=None\n",
    "evaluation_strategy=\"steps\" # \"steps\", \"epoch\"\n",
    "load_best_model_at_end=True\n",
    "metric_for_best_model=\"wer\"\n",
    "greater_is_better=False\n",
    "save_total_limit=1\n",
    "report_to=\"wandb\"\n",
    "\n",
    "num_train_epochs=2\n",
    "per_device_train_batch_size=4\n",
    "gradient_accumulation_steps=4\n",
    "gradient_checkpointing=True\n",
    "learning_rate=2e-5\n",
    "lr_scheduler_type=\"cosine\" # \"constant\", \"linear\", \"cosine\"\n",
    "warmup_ratio=0.1\n",
    "optim = \"adamw_torch\"\n",
    "weight_decay=0.01"
   ],
   "id": "61fc426f76f3fb64",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "35917c1f0e8f3b30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:13.285346Z",
     "start_time": "2024-05-17T03:00:13.283384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model ID for base model\n",
    "model_id = \"openai/whisper-large-v3\""
   ],
   "id": "375bedeb9583aa27",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:20.877127Z",
     "start_time": "2024-05-17T03:00:13.285832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load model and processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    #attn_implementation=attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True\n",
    ")"
   ],
   "id": "71aa4496f9217519",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/torch-env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:20.885373Z",
     "start_time": "2024-05-17T03:00:20.879700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the model architecture\n",
    "display(Markdown(f'```{model}```'))"
   ],
   "id": "8ed2c02ce4f9caf5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "```WhisperForConditionalGeneration(\n  (model): WhisperModel(\n    (encoder): WhisperEncoder(\n      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n      (embed_positions): Embedding(1500, 1280)\n      (layers): ModuleList(\n        (0-31): 32 x WhisperEncoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): WhisperDecoder(\n      (embed_tokens): Embedding(51866, 1280, padding_idx=50256)\n      (embed_positions): WhisperPositionalEmbedding(448, 1280)\n      (layers): ModuleList(\n        (0-31): 32 x WhisperDecoderLayer(\n          (self_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): WhisperSdpaAttention(\n            (k_proj): Linear(in_features=1280, out_features=1280, bias=False)\n            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=1280, out_features=5120, bias=True)\n          (fc2): Linear(in_features=5120, out_features=1280, bias=True)\n          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)\n)```"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "10bdc2fc57dfc32a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:20.892637Z",
     "start_time": "2024-05-17T03:00:20.889635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset ID\n",
    "dataset_id = \"mozilla-foundation/common_voice_17_0\""
   ],
   "id": "20e707e9f8a94299",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.133337Z",
     "start_time": "2024-05-17T03:00:20.893486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "dataset = DatasetDict()\n",
    "dataset[\"train\"] = load_dataset(dataset_id, language_code, split=\"train\", trust_remote_code=True)\n",
    "dataset[\"validation\"] = load_dataset(dataset_id, language_code, split=\"validation\", trust_remote_code=True)\n",
    "dataset[\"test\"] = load_dataset(dataset_id, language_code, split=\"test\", trust_remote_code=True)"
   ],
   "id": "a36a3cbea836d472",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.139945Z",
     "start_time": "2024-05-17T03:00:32.134477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset information\n",
    "dataset"
   ],
   "id": "5f0b998f9d148e0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 376\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 330\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 339\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.682204Z",
     "start_time": "2024-05-17T03:00:32.140784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset sample\n",
    "dataset[\"train\"][0]"
   ],
   "id": "e7fb9919f52c2e85",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': 'c6ea915bc1573d5253873335a07670c60dfe75546918f591b700e4a992ab7e44ab084bd752b3e69e197900cfa2e45e1f094af17209d83b5065456069e71aa84e',\n",
       " 'path': '/Users/pathfinder/.cache/huggingface/datasets/downloads/extracted/f16a9158fde2a26b92da08c939e4ccdbbedbc17acd3d4600d513b12bdad3ac73/ko_train_0/common_voice_ko_36880067.mp3',\n",
       " 'audio': {'path': '/Users/pathfinder/.cache/huggingface/datasets/downloads/extracted/f16a9158fde2a26b92da08c939e4ccdbbedbc17acd3d4600d513b12bdad3ac73/ko_train_0/common_voice_ko_36880067.mp3',\n",
       "  'array': array([ 1.13686838e-13,  6.53699317e-13,  8.81072992e-13, ...,\n",
       "         -2.78213702e-06,  1.59776391e-06,  3.08520976e-06]),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': '그 이웃을 쳐서 거짓 증거하는 사람은 방망이요 칼이요 뾰족한 살이니라',\n",
       " 'up_votes': 4,\n",
       " 'down_votes': 0,\n",
       " 'age': 'twenties',\n",
       " 'gender': 'female_feminine',\n",
       " 'accent': '서울',\n",
       " 'locale': 'ko',\n",
       " 'segment': '',\n",
       " 'variant': ''}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.700298Z",
     "start_time": "2024-05-17T03:00:32.682895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenization sample\n",
    "tokenized_sentence = processor.tokenizer(dataset[\"train\"][0][\"sentence\"])\n",
    "decoded_sentence = processor.tokenizer.decode(tokenized_sentence[\"input_ids\"])\n",
    "\n",
    "print(f\"Tokenized Sentence: {tokenized_sentence}\")\n",
    "print(f\"Decoded Sentence: {decoded_sentence}\")"
   ],
   "id": "f705e5fc7a9114aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: {'input_ids': [50258, 50364, 22069, 4329, 249, 225, 1638, 43517, 238, 2393, 3675, 1372, 241, 33830, 4285, 7116, 12211, 2124, 10006, 46407, 3946, 1206, 6639, 120, 3946, 1206, 531, 122, 108, 21799, 3049, 21155, 1129, 1425, 167, 2742, 50257], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "Decoded Sentence: <|startoftranscript|><|notimestamps|>그 이웃을 쳐서 거짓 증거하는 사람은 방망이요 칼이요 뾰족한 살이니라<|endoftext|>\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "2118b20fdd3597c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.704267Z",
     "start_time": "2024-05-17T03:00:32.701106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Down sample audio\n",
    "def preprocess_function(examples):\n",
    "    audio = examples[\"audio\"]\n",
    "    \n",
    "    examples[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=sampling_rate).input_features[0]\n",
    "    examples[\"labels\"] = processor.tokenizer(examples[\"sentence\"], return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Preprocess dataset\n",
    "dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"sentence\"], num_proc=4)"
   ],
   "id": "f9a90db19293a62c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "27f8990f04dbb239"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Inference",
   "id": "61930d88aaf1efaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.708135Z",
     "start_time": "2024-05-17T03:00:32.704949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    return_timestamps=True,\n",
    "\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    chunk_length_s=chunk_length_s,\n",
    "    batch_size=batch_size\n",
    ")"
   ],
   "id": "53dae8b4956d9ba3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def speech_recognition(audio):\n",
    "    text = pipe(audio)[\"text\"]\n",
    "    return result"
   ],
   "id": "a126faa15071f557"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:32.717920Z",
     "start_time": "2024-05-17T03:00:32.708799Z"
    }
   },
   "cell_type": "code",
   "source": "sample = dataset[\"train\"][0][\"audio\"]",
   "id": "87a9b7f3abdd502b",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:40.121834Z",
     "start_time": "2024-05-17T03:00:32.718960Z"
    }
   },
   "cell_type": "code",
   "source": "result = speech_recognition(sample)",
   "id": "44f5df47d8de171f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:40.125494Z",
     "start_time": "2024-05-17T03:00:40.122554Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "43c0e16d084c136e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ' 그 이웃을 쳐서 거짓 증거하는 사람은 방망이요 칼이요 뾰족한 살인이라',\n",
       " 'chunks': [{'timestamp': (0.0, 7.78),\n",
       "   'text': ' 그 이웃을 쳐서 거짓 증거하는 사람은 방망이요 칼이요 뾰족한 살인이라'}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "f42358f2f99e5e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def compute_metrics(pred):\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    \n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}"
   ],
   "id": "3ffcda9748086a84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:40.141575Z",
     "start_time": "2024-05-17T03:00:40.126211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    logging_dir=logging_dir,\n",
    "    save_strategy=save_strategy,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    greater_is_better=greater_is_better,\n",
    "    save_total_limit=save_total_limit,\n",
    "    report_to=report_to,\n",
    "    \n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    optim=optim,\n",
    "    weight_decay=weight_decay,\n",
    "    seed=seed\n",
    ")"
   ],
   "id": "5629ae0c796e3ccb",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T03:00:40.268303Z",
     "start_time": "2024-05-17T03:00:40.142054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "6fa46f2dc9990141",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Seq2SeqTrainer(\n\u001B[1;32m      2\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m      3\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m----> 4\u001B[0m     train_dataset\u001B[38;5;241m=\u001B[39mds,\n\u001B[1;32m      5\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mprocessor,\n\u001B[1;32m      6\u001B[0m     data_collator\u001B[38;5;241m=\u001B[39mprocessor\u001B[38;5;241m.\u001B[39mdata_collator,\n\u001B[1;32m      7\u001B[0m     compute_metrics\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m      8\u001B[0m )\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ds' is not defined"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "f50d13a926cb24ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79b5e7a906ad306d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
